{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0de05412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 1. SETUP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import ast # For parsing the list strings in the crosswalk\n",
    "\n",
    "# PATHS\n",
    "DATA_RAW = os.path.join(\"..\", \"data\", \"raw\")\n",
    "DATA_PROCESSED = os.path.join(\"..\", \"data\", \"processed\")\n",
    "os.makedirs(DATA_PROCESSED, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3633edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading Raw Data Tables...\n",
      "   ‚úÖ Tracts: (3879, 62)\n",
      "   ‚úÖ People (Combined): (157694, 287)\n",
      "   ‚úÖ SNAP Data: (149, 3)\n",
      "   ‚úÖ Relationship File: (85452, 4)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 2. LOAD RAW TABLES\n",
    "print(\"üöÄ Loading Raw Data Tables...\")\n",
    "\n",
    "# TABLE 1: ACS Tracts (The Canvas)\n",
    "t_path = os.path.join(DATA_RAW, \"acs_tract_demographics_2023.csv\")\n",
    "if os.path.exists(t_path):\n",
    "    df_tract_raw = pd.read_csv(t_path, dtype={'state':str, 'county':str, 'tract':str})\n",
    "    \n",
    "    # CLEANING: Force Padding and Standard IDs\n",
    "    df_tract_raw['state'] = df_tract_raw['state'].str.zfill(2)\n",
    "    df_tract_raw['county'] = df_tract_raw['county'].str.zfill(3)\n",
    "    df_tract_raw['tract'] = df_tract_raw['tract'].str.split('.').str[0].str.zfill(6)\n",
    "    \n",
    "    # Construct Full GEOID (11 Digits)\n",
    "    df_tract_raw['GEOID'] = df_tract_raw['state'] + df_tract_raw['county'] + df_tract_raw['tract']\n",
    "    print(f\"   ‚úÖ Tracts: {df_tract_raw.shape}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Missing: {t_path}\")\n",
    "\n",
    "# TABLE 2: ACS People (The Training Data)\n",
    "p_files = glob.glob(os.path.join(DATA_RAW, \"psam_p*.csv\"))\n",
    "if p_files:\n",
    "    df_p_raw = pd.concat([pd.read_csv(f) for f in p_files], ignore_index=True)\n",
    "    print(f\"   ‚úÖ People (Combined): {df_p_raw.shape}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Missing: psam_p*.csv\")\n",
    "\n",
    "# TABLE 3: SNAP Data (The Anchors)\n",
    "s_path = os.path.join(DATA_RAW, \"snap_county_data_clean.csv\")\n",
    "if os.path.exists(s_path):\n",
    "    df_snap_raw = pd.read_csv(s_path, dtype={'fips':str})\n",
    "    \n",
    "    # --- FIX IS HERE ---\n",
    "    # Raw FIPS is messy (e.g., '2400101 MD FSP...'). We only want the first 5 digits.\n",
    "    df_snap_raw['fips'] = df_snap_raw['fips'].astype(str).str.strip().str[:5]\n",
    "    \n",
    "    print(f\"   ‚úÖ SNAP Data: {df_snap_raw.shape}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Missing: {s_path}\")\n",
    "\n",
    "# TABLE 4: Census Relationship File (The Bridge)\n",
    "r_path = os.path.join(DATA_RAW, \"2020_Census_Tract_to_2020_PUMA.csv\")\n",
    "if os.path.exists(r_path):\n",
    "    df_rel_raw = pd.read_csv(r_path, dtype=str)\n",
    "    print(f\"   ‚úÖ Relationship File: {df_rel_raw.shape}\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Missing: {r_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a11727b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Building PUMA-County Crosswalk...\n",
      "   -> Merged 3879 tract records successfully.\n",
      "   ‚úÖ Crosswalk Built: 113 PUMAs mapped.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 3. BUILD CROSSWALK (The \"Fourth Table\" Logic)\n",
    "print(\"\\n‚öôÔ∏è Building PUMA-County Crosswalk...\")\n",
    "\n",
    "# 3.1 Prepare Relationship Data\n",
    "df_rel = df_rel_raw.copy()\n",
    "rename_map = {\n",
    "    'STATEFIP': 'STATEFP', 'COUNTYFIP': 'COUNTYFP', 'TRACT': 'TRACTCE', 'PUMA': 'PUMA5CE'\n",
    "}\n",
    "df_rel.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# Construct GEOID for Join\n",
    "df_rel['GEOID'] = df_rel['STATEFP'].str.zfill(2) + \\\n",
    "                  df_rel['COUNTYFP'].str.zfill(3) + \\\n",
    "                  df_rel['TRACTCE'].str.zfill(6)\n",
    "\n",
    "# 3.2 Merge with Tract Population (Needed for Weights)\n",
    "df_merged = pd.merge(df_rel, df_tract_raw[['GEOID', 'total_population']], on='GEOID', how='inner')\n",
    "\n",
    "if len(df_merged) == 0:\n",
    "    print(\"‚ùå CRITICAL ERROR: Merge failed. Check GEOID formats.\")\n",
    "else:\n",
    "    print(f\"   -> Merged {len(df_merged)} tract records successfully.\")\n",
    "\n",
    "# 3.3 Aggregate to PUMA Level\n",
    "crosswalk_rows = []\n",
    "for puma, group in df_merged.groupby('PUMA5CE'):\n",
    "    total_puma_pop = group['total_population'].sum()\n",
    "    unique_counties = group['COUNTYFP'].unique().tolist()\n",
    "    state_code = group['STATEFP'].iloc[0] # Get State code for this PUMA\n",
    "    \n",
    "    county_weights = []\n",
    "    for county in unique_counties:\n",
    "        c_pop = group[group['COUNTYFP'] == county]['total_population'].sum()\n",
    "        w = c_pop / total_puma_pop if total_puma_pop > 0 else 0\n",
    "        county_weights.append(w)\n",
    "        \n",
    "    crosswalk_rows.append({\n",
    "        'PUMA': puma,\n",
    "        'STATEFP': state_code,\n",
    "        'counties': unique_counties,\n",
    "        'weights': county_weights\n",
    "    })\n",
    "\n",
    "df_crosswalk = pd.DataFrame(crosswalk_rows)\n",
    "print(f\"   ‚úÖ Crosswalk Built: {len(df_crosswalk)} PUMAs mapped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "317017ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßÆ Calculating Weighted SNAP Rates...\n",
      "   ‚úÖ Calculated SNAP Rates for 113 PUMAs.\n",
      "   -> Max Rate: 24.39%\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 4. CALCULATE LOCAL SNAP RATES\n",
    "print(\"\\nüßÆ Calculating Weighted SNAP Rates...\")\n",
    "\n",
    "# 4.1 Calculate Raw County SNAP Rates\n",
    "# Denominator: Sum Tract Population by County (using clean FIPS)\n",
    "df_tract_raw['GEOID_COUNTY'] = df_tract_raw['state'] + df_tract_raw['county']\n",
    "county_total_pop = df_tract_raw.groupby('GEOID_COUNTY')['total_population'].sum()\n",
    "\n",
    "# Numerator: SNAP Counts (indexed by clean 5-digit FIPS)\n",
    "snap_map = {}\n",
    "for idx, row in df_snap_raw.iterrows():\n",
    "    fips = row['fips'] # This is now clean '24001'\n",
    "    count = row['snap_persons_total']\n",
    "    \n",
    "    pop = county_total_pop.get(fips, 0)\n",
    "    rate = count / pop if pop > 0 else 0\n",
    "    snap_map[fips] = rate\n",
    "\n",
    "# 4.2 Calculate Weighted PUMA Rates\n",
    "puma_rates = []\n",
    "for idx, row in df_crosswalk.iterrows():\n",
    "    puma = row['PUMA']\n",
    "    state = row['STATEFP']\n",
    "    counties = row['counties'] \n",
    "    weights = row['weights']\n",
    "    \n",
    "    weighted_rate = 0.0\n",
    "    for i, cty_code in enumerate(counties):\n",
    "        full_fips = state + cty_code # '24' + '001' = '24001'\n",
    "        c_rate = snap_map.get(full_fips, 0)\n",
    "        weighted_rate += (c_rate * weights[i])\n",
    "        \n",
    "    puma_rates.append({'PUMA': puma, 'local_snap_claim_rate': weighted_rate})\n",
    "\n",
    "df_puma_rates = pd.DataFrame(puma_rates)\n",
    "print(f\"   ‚úÖ Calculated SNAP Rates for {len(df_puma_rates)} PUMAs.\")\n",
    "\n",
    "# Sanity Check\n",
    "max_rate = df_puma_rates['local_snap_claim_rate'].max()\n",
    "print(f\"   -> Max Rate: {max_rate:.2%}\")\n",
    "if max_rate == 0:\n",
    "    print(\"‚ö†Ô∏è WARNING: Rates are still 0. Check FIPS codes again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eb24e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõ†Ô∏è Enriching Tract Data...\n",
      "   ‚úÖ Final Tracts: (3879, 60)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 5. ENRICH TRACT DATA\n",
    "print(\"\\nüõ†Ô∏è Enriching Tract Data...\")\n",
    "df_tract = df_tract_raw.copy()\n",
    "\n",
    "# 5.1 Add PUMA ID (Reverse Lookup)\n",
    "tract_puma_map = df_merged[['GEOID', 'PUMA5CE']].rename(columns={'PUMA5CE': 'PUMA'})\n",
    "df_tract = df_tract.merge(tract_puma_map, on='GEOID', how='left')\n",
    "\n",
    "# 5.2 Add Local SNAP Rate\n",
    "df_tract = df_tract.merge(df_puma_rates, on='PUMA', how='left')\n",
    "\n",
    "# 5.3 Select Final Columns\n",
    "age_sex_cols = [c for c in df_tract.columns if c.startswith('m_') or c.startswith('f_')]\n",
    "race_cols = [c for c in df_tract.columns if c.startswith('race_')]\n",
    "target_cols = ['GEOID', 'GEOID_COUNTY', 'PUMA', 'local_snap_claim_rate', \n",
    "               'total_population', 'poverty_count_est', 'poverty_count_moe'] + \\\n",
    "               age_sex_cols + race_cols\n",
    "\n",
    "df_tract_final = df_tract[target_cols].copy()\n",
    "print(f\"   ‚úÖ Final Tracts: {df_tract_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0334f00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõ†Ô∏è Enriching Person Data...\n",
      "   ‚úÖ Final People: (150723, 7)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 6. ENRICH PERSON DATA\n",
    "print(\"\\nüõ†Ô∏è Enriching Person Data...\")\n",
    "df_p = df_p_raw.copy()\n",
    "\n",
    "# 6.1 Clean Target\n",
    "df_p = df_p.dropna(subset=['POVPIP'])\n",
    "df_p['is_poor'] = (df_p['POVPIP'] < 100).astype(int)\n",
    "\n",
    "# 6.2 Rename\n",
    "rename_map = {\n",
    "    'PWGTP': 'Person_Weight',\n",
    "    'AGEP': 'Age',\n",
    "    'SEX': 'Sex_Code',\n",
    "    'RAC1P': 'Race_Code',\n",
    "    'PUMA': 'PUMA'\n",
    "}\n",
    "df_p = df_p.rename(columns=rename_map)\n",
    "\n",
    "# 6.3 Standardize PUMA ID\n",
    "# PUMS often loads as int (e.g. 101) -> Needs \"00101\"\n",
    "df_p['PUMA'] = df_p['PUMA'].astype(str).str.zfill(5)\n",
    "\n",
    "# 6.4 Add Local SNAP Rate\n",
    "df_p = df_p.merge(df_puma_rates, on='PUMA', how='left')\n",
    "\n",
    "# 6.5 Select Final Columns\n",
    "p_cols = ['is_poor', 'Person_Weight', 'Age', 'Sex_Code', 'Race_Code', 'PUMA', 'local_snap_claim_rate']\n",
    "df_p_final = df_p[p_cols].copy()\n",
    "\n",
    "print(f\"   ‚úÖ Final People: {df_p_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c23e82a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving Processed Datasets...\n",
      "‚úÖ Done.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 7. SAVE TO DISK\n",
    "print(\"\\nüíæ Saving Processed Datasets...\")\n",
    "df_tract_final.to_csv(os.path.join(DATA_PROCESSED, \"df_tract_enriched.csv\"), index=False)\n",
    "df_p_final.to_csv(os.path.join(DATA_PROCESSED, \"df_person_enriched.csv\"), index=False)\n",
    "print(\"‚úÖ Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86ea17f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Recovered SNAP Rates from existing processed data.\n",
      "üõ†Ô∏è Building Oracle Dataset (Rich Features)...\n",
      "‚úÖ Saved Oracle Dataset: (150723, 9)\n",
      "   path: ../data/processed/df_person_oracle.csv\n",
      "   Features: ['is_poor', 'Person_Weight', 'Age', 'Sex_Code', 'Race_Code', 'PUMA', 'local_snap_claim_rate', 'Education_Code', 'Employment_Status']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- OPTIONAL: CREATE ORACLE DATASET (FOR BENCHMARKING) ---\n",
    "# This creates a separate \"rich\" file with Education & Employment\n",
    "# so we don't break the main pipeline.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 1. RELOAD RAW DATA (Safe Check)\n",
    "# If df_p_raw isn't in memory (due to kernel restart), reload it.\n",
    "if 'df_p_raw' not in locals():\n",
    "    print(\"üîÑ Reloading Raw Person Data...\")\n",
    "    p_files = glob.glob(os.path.join(DATA_RAW, \"psam_p*.csv\"))\n",
    "    if p_files:\n",
    "        df_p_raw = pd.concat([pd.read_csv(f) for f in p_files], ignore_index=True)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"‚ùå Could not find psam_p*.csv files.\")\n",
    "\n",
    "# 2. RECOVER CONTEXT (SNAP RATES)\n",
    "# Instead of re-calculating the crosswalk, we steal the rates from the file we just saved.\n",
    "# This ensures consistency with the main pipeline.\n",
    "main_file_path = os.path.join(DATA_PROCESSED, \"df_person_enriched.csv\")\n",
    "if os.path.exists(main_file_path):\n",
    "    # Load just PUMA and SNAP Rate\n",
    "    df_context = pd.read_csv(main_file_path, usecols=['PUMA', 'local_snap_claim_rate'], dtype={'PUMA':str})\n",
    "    df_context = df_context.drop_duplicates(subset=['PUMA'])\n",
    "    print(\"‚úÖ Recovered SNAP Rates from existing processed data.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: Main processed file missing. Run previous steps first!\")\n",
    "    df_context = pd.DataFrame(columns=['PUMA', 'local_snap_claim_rate'])\n",
    "\n",
    "# 3. BUILD ORACLE TABLE\n",
    "print(\"üõ†Ô∏è Building Oracle Dataset (Rich Features)...\")\n",
    "df_oracle = df_p_raw.copy()\n",
    "\n",
    "# A. Clean Target\n",
    "df_oracle = df_oracle.dropna(subset=['POVPIP'])\n",
    "df_oracle['is_poor'] = (df_oracle['POVPIP'] < 100).astype(int)\n",
    "\n",
    "# B. Rename & Select Key Features\n",
    "# ADDING: SCHL (Education) and ESR (Employment)\n",
    "rename_map = {\n",
    "    'PWGTP': 'Person_Weight',\n",
    "    'AGEP': 'Age',\n",
    "    'SEX': 'Sex_Code',\n",
    "    'RAC1P': 'Race_Code',\n",
    "    'PUMA': 'PUMA',\n",
    "    'SCHL': 'Education_Code',       # 1-24 scale\n",
    "    'ESR': 'Employment_Status'      # 1=Employed, 3=Unemployed, 6=Not in Labor Force\n",
    "}\n",
    "df_oracle = df_oracle.rename(columns=rename_map)\n",
    "\n",
    "# C. Standardize PUMA\n",
    "df_oracle['PUMA'] = df_oracle['PUMA'].astype(str).str.zfill(5)\n",
    "\n",
    "# D. Merge Context\n",
    "df_oracle = df_oracle.merge(df_context, on='PUMA', how='left')\n",
    "\n",
    "# E. Select Final Columns (Rich Version)\n",
    "oracle_cols = ['is_poor', 'Person_Weight', 'Age', 'Sex_Code', 'Race_Code', 'PUMA', \n",
    "               'local_snap_claim_rate', 'Education_Code', 'Employment_Status']\n",
    "\n",
    "# Filter to ensure columns exist\n",
    "final_cols = [c for c in oracle_cols if c in df_oracle.columns]\n",
    "df_oracle_final = df_oracle[final_cols].copy()\n",
    "\n",
    "# 4. SAVE SEPARATELY\n",
    "oracle_path = os.path.join(DATA_PROCESSED, \"df_person_oracle.csv\")\n",
    "df_oracle_final.to_csv(oracle_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Saved Oracle Dataset: {df_oracle_final.shape}\")\n",
    "print(f\"   path: {oracle_path}\")\n",
    "print(f\"   Features: {df_oracle_final.columns.tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
